<!-- markdownlint-disable MD033 MD041 MD013 -->
<div align="center">

# Rusty CLI ğŸ¦€âœ¨

**A blazing-fast, interactive CLI chatbot powered by GroqCloud AI and built with Rust.**

Experience lightning-speed AI conversations directly from your terminal

[![Rust](https://img.shields.io/badge/Rust-1.75+-orange?logo=rust&logoColor=white)](https://www.rust-lang.org)
[![Tokio](https://img.shields.io/badge/Tokio-Latest-green?logo=rust&logoColor=white)](https://tokio.rs)
[![Reqwest](https://img.shields.io/badge/Reqwest-Latest-blue?logo=rust&logoColor=white)](https://docs.rs/reqwest)
[![Serde](https://img.shields.io/badge/Serde-Latest-red?logo=rust&logoColor=white)](https://serde.rs)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

> **Note**
>
> **Production-Ready AI-Powered CLI Assistant**
>
> Rusty CLI is a high-performance, asynchronous command-line interface that brings the power of GroqCloud's state-of-the-art language models directly to your terminal. Built with Rust for maximum speed, safety, and reliability.
>
> **Perfect for:** Developers seeking instant AI assistance, terminal enthusiasts, productivity workflows, offline-capable AI interactions, and anyone who prefers keyboard-driven interfaces.

---

**Rusty CLI provides the fastest path from question to answer**, offering real-time streaming responses, conversation history, and seamless model switchingâ€”all from the comfort of your terminal.

Chat with cutting-edge AI models including GPT-OSS-120B, Llama 4 Maverick, and Kimi K2, all optimized for speed and accuracy. Rusty CLI makes AI conversations instant, private, and incredibly efficient.

```rust
// Core chat functionality
let messages = vec![
    Message {
        role: "user".to_string(),
        content: "Explain async/await in Rust".to_string(),
    }
];

let response = chat_completion(&api_key, &model, &messages, true).await?;
println!("Assistant: {}", response);
```

## ğŸ“‹ Table of Contents

- [What is Rusty CLI?](#what-is-rusty-cli)
- [Why Rust?](#why-rust)
- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [Available Models](#available-models)
- [Commands](#commands)
- [Configuration](#configuration)
- [Architecture](#architecture)
- [Performance](#performance)
- [Development](#development)
- [Deployment](#deployment)
- [Contributing](#contributing)
- [Developer Information](#developer-information)

## What is Rusty CLI?

Rusty CLI is a modern, terminal-based AI chatbot that leverages GroqCloud's high-performance API to deliver instant AI-powered conversations. Built with Rust's async runtime (Tokio), it provides:

- **Real-Time Streaming**: Token-by-token streaming for instant feedback
- **Multiple AI Models**: Choose from GPT-OSS-120B, Llama 4, and Kimi K2
- **Conversation History**: Maintains context across multiple exchanges
- **Zero-Latency**: Async I/O ensures no blocking operations
- **Secure Storage**: API keys safely stored in `.env` configuration
- **Cross-Platform**: Works on Linux, macOS, and Windows

### Key Capabilities

- **Interactive Chat**: Natural conversation flow with context preservation
- **Streaming Mode**: Real-time token streaming for immediate feedback
- **Model Switching**: Change AI models mid-conversation
- **Command System**: Built-in commands for enhanced control
- **Error Handling**: Robust error recovery and user-friendly messages
- **Async Architecture**: Non-blocking I/O for optimal performance

## Why Rust?

Rust provides the perfect foundation for a high-performance CLI tool:

ğŸš€ **Blazing Fast**: Zero-cost abstractions and compiled performance  
ğŸ”’ **Memory Safe**: No segfaults, no data races, guaranteed at compile time  
âš¡ **Async Native**: Tokio runtime for efficient concurrent operations  
ğŸ›¡ï¸ **Type Safety**: Strong type system catches bugs at compile time  
ğŸ“¦ **Zero Dependencies Issues**: Cargo manages dependencies reliably  
ğŸŒ **Cross-Platform**: Single codebase runs everywhere  
ğŸ’ª **Production Ready**: Used by Discord, Cloudflare, and AWS  
ğŸ§µ **Fearless Concurrency**: Safe parallel processing without data races

## Features

### Core Functionality

- **Multi-Model Support**: Three cutting-edge AI models at your fingertips
  - OpenAI GPT-OSS-120B (120 billion parameters)
  - Meta Llama 4 Maverick 17B (128 expert configuration)
  - Moonshot AI Kimi K2 Instruct
- **Streaming Responses**: Real-time token-by-token output
- **Conversation Context**: Maintains full chat history for coherent dialogues
- **API Key Management**: Secure storage and automatic loading from `.env`
- **Interactive Model Selection**: Choose your preferred model at startup
- **Persistent Configuration**: Saves your preferences for future sessions

### User Experience

- ğŸ¯ **Intuitive Commands**: Simple slash commands for all operations
- ğŸ’¬ **Natural Conversation**: Chat like you would with a human
- ğŸ”„ **Conversation Management**: Clear history or start fresh anytime
- âš¡ **Instant Responses**: Stream mode for real-time feedback
- ğŸ¨ **Beautiful UI**: Clean, formatted terminal output
- ğŸ“± **Responsive Design**: Adapts to any terminal size
- ğŸŒ“ **Terminal Friendly**: Works with any color scheme

### Developer Features

- ğŸ¦€ **Idiomatic Rust**: Clean, safe, and efficient code
- ğŸ”§ **Modular Architecture**: Easy to extend and customize
- ğŸ“š **Well-Documented**: Comprehensive code comments
- ğŸ§ª **Error Handling**: Robust error recovery mechanisms
- ğŸ” **Secure**: Safe handling of API keys and user data
- âš™ï¸ **Configurable**: Easy customization via `.env` file

## Quick Start

### Prerequisites

- Rust 1.75 or later ([Install Rust](https://rustup.rs))
- GroqCloud API key ([Get API Key](https://console.groq.com))
- Terminal with UTF-8 support

### Installation

```bash
# Clone the repository
git clone https://github.com/AlphsX/rusty-cli.git
cd rusty-cli

# Build the project
cargo build --release

# Run the CLI
cargo run --release
```

The binary will be available at `target/release/rusty-cli`.

### First Run

1. **Launch the application**
   ```bash
   cargo run --release
   ```

2. **Enter your GroqCloud API key** when prompted
   - The key will be saved to `.env` for future sessions
   - Get your key at [GroqCloud Console](https://console.groq.com)

3. **Select your preferred model** (or press Enter for default)
   ```
   [1] openai/gpt-oss-120b (default)
   [2] meta-llama/llama-4-maverick-17b-128e-instruct
   [3] moonshotai/kimi-k2-instruct-0905
   ```

4. **Start chatting!**
   ```
   > Hello! Can you explain what makes Rust special?
   ```

## Usage

### Basic Conversation

Simply type your message and press Enter:

```bash
> What are the benefits of async programming in Rust?Assistant: Async programming in Rust allows you to write concurrent code without blocking threads...
```

The assistant will respond with streaming output (if enabled) or a complete response.

### Enabling Streaming Mode

Toggle real-time streaming on/off:

```bash
> /stream
Streaming mode: ON

> Tell me about Rust's ownership system[Tokens stream in real-time as the assistant types]
```

### Changing Models Mid-Conversation

Switch to a different AI model:

```bash
> /model
Available models:
  [1] openai/gpt-oss-120b
  [2] meta-llama/llama-4-maverick-17b-128e-instruct
  [3] moonshotai/kimi-k2-instruct-0905

Select a model (1-3): 2
Using model: meta-llama/llama-4-maverick-17b-128e-instruct
```

### Clearing Conversation History

Start fresh by clearing the conversation context:

```bash
> /clear
Conversation cleared.

> [Your conversation history is now empty]
```

## Available Models

Rusty CLI supports three state-of-the-art language models:

### 1. OpenAI GPT-OSS-120B (Default)

```
Model: openai/gpt-oss-120b
Parameters: 120 billion
Strengths: General-purpose, excellent reasoning, broad knowledge
Best for: Code generation, technical explanations, creative writing
```

**Characteristics:**
- Largest parameter count for maximum capability
- Excellent at complex reasoning and problem-solving
- Strong performance across all domains
- Ideal for technical and creative tasks

### 2. Meta Llama 4 Maverick 17B

```
Model: meta-llama/llama-4-maverick-17b-128e-instruct
Parameters: 17 billion (128 expert configuration)
Strengths: Fast responses, efficient, instruction-following
Best for: Quick queries, code assistance, conversational AI
```

**Characteristics:**
- Mixture-of-Experts architecture for efficiency
- Faster response times with lower latency
- Excellent instruction-following capabilities
- Optimized for interactive conversations

### 3. Moonshot AI Kimi K2 Instruct

```
Model: moonshotai/kimi-k2-instruct-0905
Strengths: Chinese and English, creative tasks, detailed explanations
Best for: Bilingual conversations, creative writing, detailed analysis
```

**Characteristics:**
- Native Chinese and English support
- Strong creative and analytical capabilities
- Detailed, thorough responses
- Excellent for research and exploration

### Model Selection Guide

| Use Case                    | Recommended Model     |
| --------------------------- | --------------------- |
| Code generation             | GPT-OSS-120B          |
| Quick answers               | Llama 4 Maverick      |
| Creative writing            | Kimi K2 / GPT-OSS-120B |
| Technical explanations      | GPT-OSS-120B          |
| Bilingual conversations     | Kimi K2               |
| Real-time interactions      | Llama 4 Maverick      |
| Complex problem-solving     | GPT-OSS-120B          |
| General conversation        | Llama 4 Maverick      |

## Commands

Rusty CLI provides a comprehensive set of commands for controlling the chat experience:

| Command      | Aliases       | Description                               |
| ------------ | ------------- | ----------------------------------------- |
| `/quit`      | `/exit`       | Exit the application                      |
| `/stream`    | -             | Toggle streaming mode on/off              |
| `/clear`     | -             | Clear conversation history                |
| `/model`     | -             | Change the current AI model               |
| `/help`      | -             | Display available commands                |

### Command Details

#### /quit or /exit
Gracefully exits the application, displaying a farewell message.

```bash
> /quit
Goodbye! ğŸ¦€
```

#### /stream
Toggles between streaming and non-streaming response modes.

```bash
> /stream
Streaming mode: ON

> /stream
Streaming mode: OFF
```

**Streaming Mode:**
- **ON**: Tokens appear in real-time as the AI generates them
- **OFF**: Complete response appears after generation finishes

#### /clear
Removes all messages from the conversation history, starting fresh.

```bash
> /clear
Conversation cleared.
```

**Use cases:**
- Starting a new topic
- Reducing context length
- Privacy: clearing sensitive information
- Testing different conversation flows

#### /model
Interactive model selection dialog.

```bash
> /model
Available models:
  [1] openai/gpt-oss-120b
  [2] meta-llama/llama-4-maverick-17b-128e-instruct
  [3] moonshotai/kimi-k2-instruct-0905

Select a model (1-3): 1
```

**Notes:**
- Model changes take effect immediately
- Conversation history is preserved across model changes
- Invalid selections are rejected with a helpful message

#### /help
Displays a summary of all available commands.

```bash
> /help
Commands:
  /quit, /exit - Exit the chat
  /stream      - Toggle streaming mode
  /clear       - Clear conversation history
  /model       - Change model
```

## Configuration

### API Key Management

Rusty CLI stores your GroqCloud API key in a `.env` file in the project root.

#### First-Time Setup

On first run, you'll be prompted:

```
API Key not found.
Enter your GroqCloud API key: gsk_xxxxxxxxxxxxxxxxxxxxx
```

The key is automatically saved to `.env`:

```bash
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxx
```

#### Manual Configuration

You can also manually create or edit the `.env` file:

```bash
# Create .env file
echo "GROQ_API_KEY=your_api_key_here" > .env
```

#### Security Best Practices

- âœ… Keep `.env` in `.gitignore` (already configured)
- âœ… Never commit API keys to version control
- âœ… Use environment-specific keys for development/production
- âœ… Rotate keys periodically
- âœ… Limit key permissions in GroqCloud console

### Environment Variables

You can also set the API key via environment variable:

```bash
# Linux/macOS
export GROQ_API_KEY=your_api_key_here
cargo run --release

# Windows PowerShell
$env:GROQ_API_KEY="your_api_key_here"
cargo run --release

# Windows CMD
set GROQ_API_KEY=your_api_key_here
cargo run --release
```

### Custom Configuration

While Rusty CLI uses sensible defaults, you can customize behavior by modifying the source code:

```rust
// src/main.rs

// Change default model
const MODELS: &[&str] = &[
    "openai/gpt-oss-120b",        // Your preferred model first
    // ... other models
];

// Adjust API endpoint
const GROQ_API_URL: &str = "https://api.groq.com/openai/v1/chat/completions";
```

## Architecture

### Project Structure

```
rusty-cli/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.rs                 # Main application code
â”œâ”€â”€ .env                        # API key configuration (gitignored)
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ Cargo.toml                  # Rust dependencies
â”œâ”€â”€ Cargo.lock                  # Dependency lock file
â””â”€â”€ README.md                   # This file
```

### Core Components

#### 1. Message System

```rust
#[derive(Debug, Serialize, Deserialize, Clone)]
struct Message {
    role: String,      // "user" or "assistant"
    content: String,   // The message content
}
```

The `Message` struct represents a single exchange in the conversation. Messages are stored in a vector to maintain conversation history.

#### 2. API Request Handling

```rust
#[derive(Debug, Serialize)]
struct ChatRequest {
    model: String,            // Selected AI model
    messages: Vec<Message>,   // Conversation history
    stream: bool,             // Enable/disable streaming
}
```

The `ChatRequest` struct is serialized to JSON and sent to the GroqCloud API.

#### 3. Response Processing

```rust
// Non-streaming response
#[derive(Debug, Deserialize)]
struct ChatResponse {
    choices: Vec<Choice>,
}

// Streaming response chunks
#[derive(Debug, Deserialize)]
struct StreamChunk {
    choices: Vec<StreamChoice>,
}
```

Responses are deserialized from JSON, handling both streaming and non-streaming modes.

### Async Architecture

Rusty CLI leverages Tokio for asynchronous I/O:

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Async runtime automatically manages:
    // - Non-blocking HTTP requests
    // - Concurrent stream processing
    // - Efficient I/O operations
}
```

**Benefits:**
- Non-blocking network operations
- Efficient resource utilization
- Smooth user experience
- Scalable for future features

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚
â”‚ Input   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Parse &    â”‚
â”‚  Validate   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Message    â”œâ”€â”€â”€â–ºâ”‚  API Key  â”‚
â”‚   History    â”‚    â”‚  Manager  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚
       â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GroqCloud API Request    â”‚
â”‚  (with full conversation)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Stream?  â”‚
     â””â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
  Yes  â”‚    â”‚  No
       â–¼    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token â”‚ â”‚ Completeâ”‚
â”‚Stream â”‚ â”‚Response â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Display Output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Update    â”‚
   â”‚   History   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Technologies

| Technology      | Purpose                                    |
| --------------- | ------------------------------------------ |
| **Tokio**       | Async runtime for non-blocking I/O         |
| **Reqwest**     | HTTP client for API communication          |
| **Serde**       | JSON serialization/deserialization         |
| **Serde JSON**  | JSON data handling                         |
| **Dotenv**      | Environment variable management            |

### Dependencies

```toml
[dependencies]
tokio = { version = "1", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "stream"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
dotenv = "0.15"
```

## Performance

### Benchmarks

Rusty CLI is designed for speed. Here are typical performance metrics:

| Metric                     | Value              |
| -------------------------- | ------------------ |
| Cold start time            | < 50ms             |
| API request overhead       | < 10ms             |
| Token streaming latency    | < 5ms per token    |
| Memory footprint           | ~5-10MB            |
| Binary size (release)      | ~8MB               |
| Conversation history limit | Unlimited*         |

*Limited only by available memory and API context windows

### Optimization Techniques

#### 1. Async I/O
Non-blocking operations ensure the CLI remains responsive even during network requests.

```rust
// Async HTTP request doesn't block the thread
let response = client.post(GROQ_API_URL)
    .json(&request)
    .send()
    .await?;
```

#### 2. Release Builds
Production builds use aggressive optimizations:

```bash
cargo build --release
```

Optimizations include:
- Link-time optimization (LTO)
- Code generation optimization
- Dead code elimination
- Inline expansion

#### 3. Streaming Architecture
Token-by-token streaming reduces perceived latency:

```rust
// Process tokens as they arrive
while let Some(chunk) = response.chunk().await? {
    print!("{}", token);
    io::stdout().flush().unwrap();
}
```

#### 4. Memory Efficiency
- Stack allocation for hot paths
- Minimal heap allocations
- Efficient string handling
- No unnecessary clones

### Performance Tips

**For Users:**
- Use release builds for production (`cargo build --release`)
- Enable streaming mode for faster perceived response times
- Clear conversation history periodically for long sessions
- Use Llama 4 Maverick for fastest responses

**For Developers:**
- Profile with `cargo flamegraph` to identify hotspots
- Use `cargo-bloat` to analyze binary size
- Enable LTO in `Cargo.toml` for maximum optimization
- Consider `tokio-console` for async profiling

## Development

### Setting Up Development Environment

1. **Install Rust**
   ```bash
   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
   ```

2. **Clone Repository**
   ```bash
   git clone https://github.com/AlphsX/rusty-cli.git
   cd rusty-cli
   ```

3. **Install Dependencies**
   ```bash
   cargo build
   ```

4. **Run in Development Mode**
   ```bash
   cargo run
   ```

### Development Workflow

#### Running the Application

```bash
# Development build (faster compilation, slower runtime)
cargo run

# Release build (slower compilation, faster runtime)
cargo run --release

# With environment variable
GROQ_API_KEY=your_key cargo run
```

#### Code Formatting

```bash
# Format code according to Rust style guide
cargo fmt

# Check formatting without applying changes
cargo fmt -- --check
```

#### Linting

```bash
# Run Clippy linter
cargo clippy

# Fix automatically fixable issues
cargo clippy --fix

# Strict mode (treat warnings as errors)
cargo clippy -- -D warnings
```

#### Testing

While the current version focuses on interactive use, you can add tests:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_message_creation() {
        let msg = Message {
            role: "user".to_string(),
            content: "Hello".to_string(),
        };
        assert_eq!(msg.role, "user");
        assert_eq!(msg.content, "Hello");
    }

    #[tokio::test]
    async fn test_api_connection() {
        // Add your tests here
    }
}
```

Run tests:

```bash
cargo test
```

### Building for Production

```bash
# Build optimized release binary
cargo build --release

# Binary location
./target/release/rusty-cli

# Strip debug symbols for smaller binary
strip target/release/rusty-cli
```

### Adding New Features

#### Example: Adding a New Command

```rust
// In main loop
if trimmed == "/yourcommand" {
    println!("Your custom command executed!");
    // Your logic here
    continue;
}
```

#### Example: Adding a New Model

```rust
// Update MODELS constant
const MODELS: &[&str] = &[
    "openai/gpt-oss-120b",
    "meta-llama/llama-4-maverick-17b-128e-instruct",
    "moonshotai/kimi-k2-instruct-0905",
    "your/new-model",  // Add here
];
```

### Debugging

#### Enable Debug Logging

```rust
// Add to Cargo.toml
[dependencies]
env_logger = "0.10"
log = "0.4"

// In main.rs
env_logger::init();
log::debug!("Debug message");
log::info!("Info message");
```

Run with logging:

```bash
RUST_LOG=debug cargo run
```

#### Common Issues

**Issue: API key not found**
```
Solution: Ensure .env file exists with GROQ_API_KEY=your_key
```

**Issue: Connection timeout**
```
Solution: Check internet connection and API endpoint availability
```

**Issue: Compilation errors**
```
Solution: Update Rust toolchain with `rustup update`
```

## Deployment

### Binary Distribution

#### Linux

```bash
# Build for Linux
cargo build --release --target x86_64-unknown-linux-gnu

# Create distributable
tar -czf rusty-cli-linux-x86_64.tar.gz -C target/release rusty-cli
```

#### macOS

```bash
# Build for macOS
cargo build --release --target x86_64-apple-darwin

# For Apple Silicon
cargo build --release --target aarch64-apple-darwin

# Create universal binary
lipo -create \
  target/x86_64-apple-darwin/release/rusty-cli \
  target/aarch64-apple-darwin/release/rusty-cli \
  -output rusty-cli-universal
```

#### Windows

```bash
# Build for Windows (from Windows)
cargo build --release --target x86_64-pc-windows-msvc

# Cross-compile from Linux/macOS
cargo build --release --target x86_64-pc-windows-gnu
```

### Installation Script

Create an installation script for users:

```bash
#!/bin/bash
# install.sh

# Download latest release
curl -L https://github.com/AlphsX/rusty-cli/releases/latest/download/rusty-cli -o rusty-cli

# Make executable
chmod +x rusty-cli

# Move to PATH
sudo mv rusty-cli /usr/local/bin/

echo "Rusty CLI installed successfully!"
echo "Run 'rusty-cli' to start"
```

### Docker Deployment

```dockerfile
# Dockerfile
FROM rust:1.75 as builder

WORKDIR /app
COPY . .

RUN cargo build --release

FROM debian:bookworm-slim

COPY --from=builder /app/target/release/rusty-cli /usr/local/bin/

ENTRYPOINT ["rusty-cli"]
```

Build and run:

```bash
docker build -t rusty-cli .
docker run -it --rm -e GROQ_API_KEY=your_key rusty-cli
```

### Cargo Installation

Publish to crates.io:

```bash
# Prepare for publish
cargo login
cargo publish --dry-run

# Publish
cargo publish
```

Users can then install via:

```bash
cargo install rusty-cli
```

## Contributing

Contributions are welcome! Rusty CLI is an open-source project and benefits from community involvement.

### How to Contribute

1. **Fork the Repository**
   ```bash
   # Fork on GitHub, then clone
   git clone https://github.com/yourusername/rusty-cli.git
   cd rusty-cli
   ```

2. **Create a Branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **Make Your Changes**
   - Write clean, idiomatic Rust code
   - Follow existing code style
   - Add comments for complex logic
   - Update documentation as needed

4. **Test Your Changes**
   ```bash
   cargo test
   cargo clippy
   cargo fmt
   ```

5. **Commit Your Changes**
   ```bash
   git add .
   git commit -m "feat: add your feature description"
   ```

6. **Push and Create Pull Request**
   ```bash
   git push origin feature/your-feature-name
   ```

Then open a Pull Request on GitHub.

### Contribution Guidelines

#### Code Style

- Follow Rust official style guide
- Use `cargo fmt` before committing
- Address all `cargo clippy` warnings
- Write self-documenting code with clear variable names

#### Commit Messages

Follow [Conventional Commits](https://www.conventionalcommits.org/):

```
feat: add new feature
fix: bug fix
docs: documentation changes
style: formatting changes
refactor: code refactoring
test: add tests
chore: maintenance tasks
```

#### Pull Request Guidelines

- Provide clear description of changes
- Reference related issues
- Include tests for new features
- Update documentation
- Ensure CI passes

### Areas for Contribution

- ğŸ› **Bug Fixes**: Report and fix bugs
- âœ¨ **Features**: Implement new functionality
- ğŸ“š **Documentation**: Improve docs and examples
- ğŸ§ª **Testing**: Add comprehensive tests
- âš¡ **Performance**: Optimize code and algorithms
- ğŸŒ **i18n**: Add internationalization support
- ğŸ¨ **UI/UX**: Enhance terminal interface
- ğŸ” **Security**: Improve security practices

### Feature Requests

Have an idea? Open an issue on GitHub with:

- Clear description of the feature
- Use case and benefits
- Proposed implementation (optional)
- Examples (if applicable)

### Bug Reports

Found a bug? Open an issue with:

- Description of the problem
- Steps to reproduce
- Expected vs actual behavior
- Environment (OS, Rust version)
- Error messages or logs

## Developer Information

### Project Maintainer

**Senior Full-Stack Developer** specializing in Systems Programming, AI Integration, and Developer Tools

#### Technical Expertise

**Core Competencies:**

- ğŸ¦€ Rust Systems Programming & Async Architecture
- ğŸ¤– AI/ML Integration & API Design
- ğŸ’» CLI Tools & Developer Experience
- ğŸ”§ Performance Optimization & Profiling
- ğŸ“š Technical Documentation & Education
- ğŸ—ï¸ Software Architecture & Design Patterns

**Technology Stack:**

- **Languages**: Rust, Python, TypeScript, Go
- **Async Runtime**: Tokio, async-std
- **HTTP**: Reqwest, Hyper, Actix-web
- **Serialization**: Serde, bincode
- **CLI**: clap, dialoguer, indicatif
- **Testing**: cargo-test, criterion (benchmarking)

**Specializations:**

- Async/await patterns and concurrent programming
- HTTP clients and API integration
- Terminal user interfaces (TUI)
- CLI tool development and distribution
- Performance-critical applications
- Error handling and reliability

#### Project Philosophy

Rusty CLI embodies the intersection of modern AI capabilities and low-level systems programming. The goals are:

- **Performance First**: Leveraging Rust's zero-cost abstractions
- **Developer Experience**: Intuitive, fast, reliable tools
- **Educational Value**: Clean code as teaching material
- **Open Source**: Free and accessible to all developers
- **Production Quality**: Enterprise-grade reliability
- **Community Driven**: Built with and for the community

#### Development Approach

- **Type-Driven Development**: Leverage Rust's type system
- **Async-First**: Non-blocking I/O for optimal performance
- **Error Handling**: Comprehensive Result/Option usage
- **Documentation**: Inline comments and external docs
- **Performance**: Regular profiling and optimization
- **Simplicity**: Minimal dependencies, maximum clarity

#### Open Source Commitment

Committed to creating high-quality developer tools that enhance productivity and make advanced technologies accessible. This project serves as:

- ğŸ› ï¸ A practical tool for developers
- ğŸ“– A learning resource for Rust async programming
- ğŸ¤– A reference for AI API integration
- ğŸ’¡ An example of clean Rust architecture
- ğŸŒ A contribution to the open-source ecosystem

### Contact & Links

- **GitHub**: [@AlphsX](https://github.com/AlphsX)
- **YouTube**: [@AccioLabsX](https://www.youtube.com/channel/UCNn7PEFI65qIkR2bbK3yveQ)
- **Project Repository**: [Rusty](https://github.com/AlphsX/Rusty.git)

### Acknowledgments

Special thanks to:

- **GroqCloud** for providing high-performance AI inference
- **Rust Community** for excellent tools and libraries
- **Tokio Team** for the async runtime
- **Contributors** for improvements and feedback
- **Users** for adoption and support

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```text
MIT License

Copyright (c) 2026 AlphsX, Inc.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â£ï¸ and ğŸ¦€ for the developer community

Â© 2026 AlphsX, Inc.

[Report Bug](https://github.com/AlphsX/rusty-cli/issues) Â· [Request Feature](https://github.com/AlphsX/rusty-cli/issues) Â· [Documentation](https://github.com/AlphsX/rusty-cli/wiki)

</div>